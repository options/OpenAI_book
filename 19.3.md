Chapter 19.3 tokenizer 조정 전략 (cl100k_base 등)

GPT 및 기타 대형 언어 모델은 입력된 텍스트를 내부적으로 "토큰(token)"이라는 단위로 분절하여 처리합니다. 이 절에서는 토크나이저(tokenizer)의 개념, cl100k_base와 같은 주요 토크나이저의 특징, 그리고 한국어 및 다국어 환경에서 토크나이저를 조정하고 최적화하는 전략에 대해 자세히 설명합니다.

📌 목차:

- 19.3.1 토크나이저란 무엇인가?
- 19.3.2 GPT에서 사용되는 토크나이저 종류
- 19.3.3 cl100k_base의 특징 및 구조 이해
- 19.3.4 한글 처리에서의 토크나이저 문제
- 19.3.5 토크나이징 최적화 전략
- 19.3.6 실습: 동일 텍스트의 토큰 길이 비교
- 19.3.7 사용자 정의 토크나이저 설계 가능성과 대안

 

19.3.1 토크나이저란 무엇인가?

토크나이저는 자연어를 모델이 이해할 수 있는 단위인 "토큰"으로 분해하는 전처리 단계의 핵심 구성요소입니다. 단어, 부분 단어, 심지어는 단일 문자까지 다양한 방식으로 텍스트를 분할할 수 있으며, 각 토큰은 모델 내부에서 고유한 ID로 변환되어 사용됩니다.

예: "OpenAI는 재미있는 모델입니다."

- 단어 단위 토크나이저 → ["OpenAI", "는", "재미있는", "모델입니다", "."]
- GPT 토크나이저 (BPE 기반) → ["Open", "AI", "는", "재", "미", "있", "는", "모", "델", "입니다", "."]

GPT 모델은 대부분 BPE(Byte Pair Encoding) 혹은 유사한 압축 기반 토크나이징 알고리즘을 따릅니다. 이는 희귀 단어도 조각(token)으로 쪼개어 인코딩할 수 있게 해줍니다.

 

19.3.2 GPT에서 사용되는 토크나이저 종류

OpenAI는 GPT 모델에 따라 서로 다른 토크나이저를 사용해 왔으며, 다음과 같은 이름의 토크나이저들이 주로 활용됩니다:

| 모델                        | 사용 토크나이저          | 특징                                                                 |
|----------------------------|--------------------------|----------------------------------------------------------------------|
| GPT-2                      | gpt2                     | 단어 기반 BPE / 영어 등 라틴 문자에 유리                          |
| GPT-3 (davinci 계열)       | p50k_base                | 다양한 언어 지원, GPT-2보다 범용적                                 |
| Codex                      | p50k_base                | 코드 최적화                                                        |
| GPT-3.5 (text-davinci-003) | p50k_base                | 일반 목적 언어 모델                                                |
| GPT-3.5-turbo (최신)       | cl100k_base              | 속도 최적화, 다국어 최적화, context 길이 개선                      |
| GPT-4                      | cl100k_base              | 최신 사양, 높은 압축률, 멀티모달 환경 대응                         |
| GPT-4o                     | cl100k_base              | 토큰 효율 극대화, 여러 언어의 음소/형태소 분해 최적화            |

여기서 가장 중요한 것은 cl100k_base로, GPT-4 및 GPT-4o에서 사용되는 최신 토크나이저입니다.

 

19.3.3 cl100k_base의 특징 및 구조 이해

cl100k_base는 다음과 같은 여러 장점을 가진 고성능 토크나이저입니다:

- Byte-level BPE 알고리즘 적용: 낮은 수준에서 문자 또는 바이트 단위로 분석해 희귀 언어에도 유연하게 대응합니다.
- 압축률이 높음: 동일 문장을 더 적은 수의 토큰으로 인코딩 가능 → 비용/속도 절감
- 멀티언어 지원 최적화: 한국어, 일본어, 중국어 등 형태소 기반 언어에서 호환성 향상
- 컨텍스트 길이 확장 대비: 최대 128k 토큰까지 안정적으로 분할이 가능함

예제 비교:

한국어 문장: “안녕하세요. 저는 AI 개발자입니다.”

- gpt2 토크나이저: 약 15~18 토큰
- p50k_base: 약 12~14 토큰
- cl100k_base: 약 10~12 토큰

이처럼 cl100k_base는 비교적으로 한국어를 더 효율적으로 처리합니다.

OpenAI는 이 토크나이저를 위해 tiktoken이라는 라이브러리를 제공합니다.

🌐 설치:  
pip install tiktoken

 

19.3.4 한글 처리에서의 토크나이저 문제

한국어는 영어와 달리 다음과 같은 특징을 갖습니다:

- 조사, 어미, 접사가 풍부 → 단어의 변화가 많음
- 단어 간 구분을 명확히 하기 어려운 경우가 많음
- 띄어쓰기 오류 허용률이 낮음 → 토크나이징에 민감

예: "정보를" → "정보" + "를" 로 나눠지지 않으면 인해 의미 손실 가능

특히, 과거 GPT-2, GPT-3에서 사용된 BPE 기반 토크나이저는 형태소 분석에 최적화되어 있지 않아 한국어 내용을 부자연스럽게 토큰화하는 문제가 많았습니다. 예를 들어 “안녕하세요”를 “안”, “녕”, “하”, “세”, “요”로 토큰화하면서 실제 의미 단위가 깨지거나, 동일한 단어가 매번 다른 조합으로 처리되어 Embedding 일관성에도 영향을 미칠 수 있습니다.

 

19.3.5 토크나이징 최적화 전략

OpenAI API를 사용할 때, 불필요한 토큰 낭비를 방지하고 처리 속도 및 비용을 줄이기 위한 토크나이저 최적화 전략은 다음과 같습니다:

① 가급적 최신 토크나이저(cl100k_base)를 사용하는 모델 활용:

- text-embedding-3-large
- gpt-4, gpt-4-turbo, gpt-4o
- gpt-3.5-turbo-1106 이후 버전

② 반복이나 장황한 표현 축소:

- "다시 한번 말씀드리자면, 이는 매우 매우 중요한 문제입니다." (11~16 tokens)
  → "이는 중요한 문제입니다." (5~6 tokens)

③ 표 형식, 리스트 형식 사용:

- 구조화된 포맷은 압축률이 높고 토큰 효율이 좋음

④ 사전 token counting 적용:

- tiktoken 라이브러리로 실제 토큰 수를 예측하여 프롬프트 길이 조정

⑤ 불필요한 줄바꿈, 특수문자 제거 또는 일괄 정리:

- 줄바꿈마다 토큰이 증가하며, 일부 공백도 토큰으로 인식됨

⑥ 한국어의 접사 처리 고려:

- 예: “학교에”, “학교에서”, “학교로” → 어간("학교") + 조사 구조로 명시적으로 나누면 Embedding 검색 성능 향상 가능

 

19.3.6 실습: 동일 텍스트의 토큰 길이 비교

예제 문장: “오늘 날씨가 정말 좋네요!”

Python 코드를 통해 각 토크나이저의 토큰 수를 비교해보겠습니다.

import tiktoken

text = "오늘 날씨가 정말 좋네요!"

encodings = {
    "gpt2": tiktoken.get_encoding("gpt2"),
    "p50k_base": tiktoken.get_encoding("p50k_base"),
    "cl100k_base": tiktoken.get_encoding("cl100k_base")
}

for name, enc in encodings.items():
    tokens = enc.encode(text)
    print(f"{name}: {len(tokens)} tokens / {tokens}")

예상 출력:

gpt2: 14 tokens / [...]
p50k_base: 10 tokens / [...]
cl100k_base: 7 tokens / [...]

이처럼 같은 문장이라도 토크나이저에 따라 차이가 있으며,
최신 cl100k_base 사용 시 가장 효율적인 토크나이징이 이루어집니다.

 

19.3.7 사용자 정의 토크나이저 설계 가능성과 대안

현재 OpenAI API는 커스텀 토크나이저를 직접 사용할 수는 없지만, 다음과 같은 전략은 가능합니다:

- 사용자 입력 전처리를 통해 의미 단위(형태소)로 분할 처리
- KoNLPy, Khaiii 등 한국어 형태소 분석기와 함께 구조화
  → 사용자 입력: “사과를 먹었다”
  → 분석 결과: [“사과”, “를”, “먹다”, “았”, “다”] → 재조합 후 프롬프트 구성

- 토큰 절약 목적이라면 embedding 저장 시 텍스트 압축 혹은 공통 어미 제거 등 전처리 필요

또한 embedding 용도로는 sentence-transformers 등 open-source 모델에서 huggingface tokenizer 등을 커스터마이징하여 별도 사용 가능함

 

🔍 요약 정리

- cl100k_base는 OpenAI의 최신 토크나이저로 한국어에 비교적 최적화됨
- 토크나이저의 차이는 토큰 수 (= 비용), 품질, 성능에 큰 영향을 미침
- tiktoken을 활용하여 프롬프트 길이와 비용을 사전에 조정할 수 있음
- 한국어는 언어 특성상 접사, 조사, 형식 변화가 많아 사전 전처리 전략이 중요
- 구조화된 출력 방식과 절제된 언어 스타일이 토큰 최적화에 도움됨

다음 장에서는 이러한 토큰 전략을 실제 챗봇의 프롬프트 구성 방식에 어떻게 적용할 수 있는지 구체적인 사용자 시나리오 중심으로 알아보겠습니다.