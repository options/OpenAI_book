Chapter 12.3 RAG (Retrieval-Augmented Generation) êµ¬í˜„

ì´ë²ˆ ì ˆì—ì„œëŠ” ìƒì„±í˜• AIì˜ í•µì‹¬ í™œìš© ê¸°ë²• ì¤‘ í•˜ë‚˜ì¸ RAG(Retrieval-Augmented Generation)ì˜ êµ¬ì¡°ì™€ í™œìš©ë²•ì„ ê¹Šì´ ìˆê²Œ ë‹¤ë£¨ê³ , LangChain ë° LlamaIndex ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ì—°ê³„í•œ ì‹¤ì „ êµ¬í˜„ ë°©ë²•ê¹Œì§€ ë‹¨ê³„ì ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤. RAGëŠ” ì™¸ë¶€ ë¬¸ì„œë¥¼ í†µí•´ ì§€ì‹ì„ ë³´ì™„í•˜ëŠ” ìƒì„± ë°©ì‹ìœ¼ë¡œ, ì •í™•ì„±ê³¼ ì‹ ë¢°ì„±ì„ ìš”êµ¬í•˜ëŠ” ë¹„ì¦ˆë‹ˆìŠ¤ ì‘ìš©ì— íŠ¹íˆ ê°•ë ¥í•œ ë„êµ¬ì…ë‹ˆë‹¤.

ğŸ™‹â€ ê¸°ë³¸ ê°œë…: RAGë€ ë¬´ì—‡ì¸ê°€?

RAG(Retrieval-Augmented Generation)ëŠ” ì™¸ë¶€ ì§€ì‹ë² ì´ìŠ¤ë‚˜ ë¬¸ì„œì—ì„œ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ê²€ìƒ‰(Retrieval) ë‹¨ê³„ì™€, í•´ë‹¹ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ì„ ìƒì„±í•˜ëŠ” ìƒì„±(Generation)ì„ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ë°©ë²•ë¡ ì…ë‹ˆë‹¤. ì´ëŠ” ë‹¨ìˆœíˆ LLM(Large Language Model)ì— í”„ë¡¬í”„íŠ¸ë¥¼ ì£¼ëŠ” ìˆ˜ì¤€ì„ ë„˜ì–´ì„œ, ì •ë³´ë¥¼ ë™ì ìœ¼ë¡œ ë¶ˆëŸ¬ì™€ ë¬¸ë§¥ ì†ì— ë°˜ì˜í•¨ìœ¼ë¡œì¨ ë‹¤ìŒê³¼ ê°™ì€ ì´ì ì„ ì œê³µí•©ë‹ˆë‹¤.

- ìµœì‹  ì •ë³´ ë°˜ì˜: í›ˆë ¨ ì‹œì  ì´í›„ ìƒê¸´ ì •ë³´ë¥¼ ë‹¤ë£° ìˆ˜ ìˆìŒ
- ì •í™•ë„ í–¥ìƒ: ê¸°ì–µ ê¸°ë°˜ì´ ì•„ë‹Œ ì™¸ë¶€ ì§€ì‹ ê¸°ë°˜ ìƒì„±
- ë¹„ìš© íš¨ìœ¨ì„±: ì‚¬ì†Œí•œ ì§ˆë¬¸ì— ëŒ€ëŸ‰ í† í°ì„ ì†Œì§„í•˜ì§€ ì•ŠìŒ
- í”„ë¡¬í”„íŠ¸ ì¡±ì‡„ í•´ë°©: ëª¨ë¸ì— ëª¨ë“  ì •ë³´ë¥¼ ê³ ì •í•˜ì§€ ì•Šì•„ë„ ë¨

ğŸ§± RAGì˜ ì•„í‚¤í…ì²˜ êµ¬ì„±ë„

ì•„ë˜ëŠ” RAGì˜ ì „í˜•ì ì¸ ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤.

1. Query Input (ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì…ë ¥)
2. Query Embedding (LLM ë˜ëŠ” Embedding ëª¨ë¸ë¡œ ì…ë ¥ì„ ë²¡í„°í™”)
3. Vector Database Search (FAISS, Pinecone, Weaviate ë“±ì—ì„œ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰)
4. Context Injection (ê²€ìƒ‰ëœ ë¬¸ì„œ + ì§ˆë¬¸ â†’ í”„ë¡¬í”„íŠ¸ë¡œ êµ¬ì„±)
5. Generation (OpenAI GPT ë˜ëŠ” ë‹¤ë¥¸ LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±)

ğŸ“¦ ì£¼ìš” êµ¬ì„± ìš”ì†Œ ì„¤ëª…

| êµ¬ì„± ìš”ì†Œ | ì„¤ëª… |
|-----------|------|
| Embedding Model | í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°í™”. OpenAIì˜ text-embedding-3-small ë“± |
| Vector Database | ë²¡í„°ë¡œ ì €ì¥ëœ ë¬¸ì„œì˜ ê²€ìƒ‰. FAISS, Chroma, Pinecone |
| Retriever | ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ì°¾ëŠ” ë¡œì§ |
| Prompt Template | ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë¬¸ë§¥ì— ì‚½ì…í•˜ëŠ” í…œí”Œë¦¿ |
| LLM | OpenAI ChatGPT ëª¨ë¸ ë“±ìœ¼ë¡œ ìµœì¢… ì¶œë ¥ ìƒì„± |

ğŸ§ª ì‹¤ìŠµ: LangChainì„ ì´ìš©í•œ RAG êµ¬í˜„

ì´ì œ LangChainì„ í™œìš©í•œ RAG ì‹œìŠ¤í…œì„ ë‹¨ê³„ë³„ë¡œ ê°œë°œí•´ë³´ê² ìŠµë‹ˆë‹¤.

ğŸ”§ Step 1: í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
pip install openai langchain faiss-cpu tiktoken
```

ğŸ”§ Step 2: í™˜ê²½ ì„¤ì •

```python
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
import os

os.environ["OPENAI_API_KEY"] = "your-api-key"
```

ğŸ”§ Step 3: ë¬¸ì„œ ì„ë² ë”© ë° ë²¡í„° ìŠ¤í† ì–´ êµ¬ì„±

```python
docs = [
    "LangChain is a scaffold for building LLM-powered applications.",
    "Retrieval-Augmented Generation uses an external knowledge base to supplement the generation.",
    "OpenAI provides powerful Embedding models for vectorization.",
]

# í…ìŠ¤íŠ¸ ì„ë² ë”©
embeddings_model = OpenAIEmbeddings(model="text-embedding-3-small")  # ìµœì‹  embedding ëª¨ë¸ ì‚¬ìš©

# FAISS ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
vectorstore = FAISS.from_texts(docs, embedding=embeddings_model)
```

ğŸ”§ Step 4: Retriever + LLM êµ¬ì„± ë° QA ì²´ì¸ ìƒì„±

```python
# ê²€ìƒ‰ê¸° êµ¬ì„±
retriever = vectorstore.as_retriever()

# ì–¸ì–´ëª¨ë¸ ì„¤ì •
llm = ChatOpenAI(model_name="gpt-4", temperature=0)

# RAG ì²´ì¸ êµ¬ì„±
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",  # ê²€ìƒ‰ ë¬¸ì„œë“¤ì„ "stuffing"í•˜ì—¬ í”„ë¡¬í”„íŠ¸ì— ë„£ëŠ” ê°„ë‹¨í•œ ë°©ì‹
    retriever=retriever,
    return_source_documents=True
)
```

ğŸ”§ Step 5: ì§ˆë¬¸ ì…ë ¥ ë° ì‹¤í–‰

```python
result = qa_chain("What is Retrieval-Augmented Generation?")
print("Answer:", result["result"])
print("\nğŸ”— Source Document(s):")
for doc in result["source_documents"]:
    print("-", doc.page_content)
```

ğŸ§  Prompt ë‚´ë¶€ êµ¬ì„± ì˜ˆì‹œ

LangChain ë‚´ë¶€ì ìœ¼ë¡œ ì•„ë˜ì™€ ê°™ì€ í˜•ì‹ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.

```
You are a helpful assistant.
Use the following pieces of context to answer the user's question.

Context:
[ë¬¸ì„œ1]
[ë¬¸ì„œ2]

Question: What is Retrieval-Augmented Generation?
Answer:
```

ì´ì²˜ëŸ¼ ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ìµœëŒ€ ì…ë ¥ í† í° ë²”ìœ„ ë‚´ì—ì„œ LLM í”„ë¡¬í”„íŠ¸ì— ì§ì ‘ ì‚½ì…í•˜ëŠ” ë°©ì‹ì€ â€œstuffing ë°©ì‹â€ì´ë¼ í•˜ë©°, ë‹¤ì–‘í•œ êµ¬ì„± ì „ëµ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.

ğŸ“ˆ RAG êµ¬ì„± ì „ëµ ë¹„êµ

| ì „ëµ | ì„¤ëª… | ì¥ì  | ë‹¨ì  |
|------|------|------|------|
| Stuffing | ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê·¸ëŒ€ë¡œ í”„ë¡¬í”„íŠ¸ì— ë¶™ì„ | ë‹¨ìˆœ, ë¹ ë¦„ | í† í° ì´ˆê³¼ ì‹œ ìœ ì‹¤ |
| Map-Reduce | ê° ë¬¸ì„œì— ëŒ€í•´ ê°œë³„ ì‘ë‹µ â†’ ì§‘ê³„ | ëŒ€ê·œëª¨ ë¬¸ì„œì—ì„œ ì•ˆì •ì  | ë¹„ìš© â†‘, ì†ë„ â†“ |
| Refine | ì´ˆê¸° ë¬¸ì„œë¡œ ì‘ë‹µ ìƒì„± í›„ ì ì§„ì  ê°œì„  | ê³ í’ˆì§ˆ ìƒì„± | ì„¤ê³„ ë³µì¡, ë¹„ìš© â†‘ |

LangChainì€ ìœ„ ì „ëµ ëª¨ë‘ë¥¼ chain_type ì¸ìë¡œ ì§€ì›í•©ë‹ˆë‹¤.

```python
RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="map_reduce",  # ë˜ëŠ” "refine"
    retriever=retriever,
)
```

ğŸ“™ LlamaIndexë¥¼ í™œìš©í•œ ëŒ€ì•ˆ êµ¬í˜„

LlamaIndexëŠ” ë¬¸ì„œ êµ¬ì¡°í™”ì— ë” ì¤‘ì ì„ ë‘” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ë¬¸ì„œ â†’ ì¸ë±ìŠ¤ â†’ ì¿¼ë¦¬ íë¦„ìœ¼ë¡œ êµ¬ì„±ë˜ë©°, êµ¬ì¡°ì  ë©”íƒ€ë°ì´í„°ë‚˜ ê¸´ ë¬¸ì„œ ì²˜ë¦¬ì— ë” ìœ ë¦¬í•©ë‹ˆë‹¤.

ğŸ”§ í•µì‹¬ ì½”ë“œ êµ¬ì¡° ì˜ˆì‹œ

```python
from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext
from llama_index.llms import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding

# ë¬¸ì„œ ë¡œë”©
documents = SimpleDirectoryReader("./docs").load_data()

# ì„œë¹„ìŠ¤ ì»¨í…ìŠ¤íŠ¸ ì„¤ì •
service_context = ServiceContext.from_defaults(
    llm=OpenAI(model="gpt-4"),
    embed_model=OpenAIEmbedding(model="text-embedding-3-small")
)

# ì¸ë±ìŠ¤ ë¹Œë“œ
index = VectorStoreIndex.from_documents(documents, service_context=service_context)

# ì§ˆì˜ ë° ë‹µë³€ ìƒì„±
query_engine = index.as_query_engine()
response = query_engine.query("RAGì˜ ì¥ì ì€ ë¬´ì—‡ì¸ê°€ìš”?")
print(response)
```

ğŸ“Œ LlamaIndex vs LangChain ë¹„êµ

| í•­ëª© | LangChain | LlamaIndex |
|------|-----------|------------|
| ì„¤ê³„ ì¤‘ì‹¬ | ì²´ì¸, í”„ë¡¬í”„íŠ¸ êµ¬ì„± ì¤‘ì‹¬ | ë¬¸ì„œ ì¸ë±ì‹± ì¤‘ì‹¬ |
| ìœ ì—°ì„± | ë‹¤ì–‘í•œ ì „ëµ êµ¬ì„± ê°€ëŠ¥ | ë¬¸ì„œ ê¸°ë°˜ ì›Œí¬í”Œë¡œì— ì í•© |
| íŠœë‹ ë‚œì´ë„ | ì¤‘ | ë‚®ìŒ |
| ë¬¸ì„œ ê¸¸ì´ ì²˜ë¦¬ | stuffing í•œê³„ | chunking ë‚´ì¥ ë” ê°•ë ¥ |

ğŸ“ Best Practices: ì‹¤ì „ RAG ì ìš© ì‹œ ì£¼ì˜í•  ì 

1. Embedding ëª¨ë¸ ì„ íƒ
   - í´ë¼ì´ì–¸íŠ¸ ìš”ì²­ì´ ë‹¤ì–‘í•  ê²½ìš° ë‹¤êµ­ì–´ ì§€ì›ë˜ëŠ” ëª¨ë¸ ì‚¬ìš©
   - OpenAI text-embedding-3-smallì€ ë¹„ìš© íš¨ìœ¨ì  ì‚¬ìš© ê°€ëŠ¥

2. Chunk size ì¡°ì ˆ
   - ê²€ìƒ‰ ë‹¨ìœ„ê°€ ë„ˆë¬´ í¬ë©´ ê´€ë ¨ì„± â†“, ë„ˆë¬´ ì‘ìœ¼ë©´ ë¬¸ë§¥ì„± â†“
   - ì¼ë°˜ì ìœ¼ë¡œ 300~500 tokenì´ ì¶”ì²œë¨

3. Chunk overlap ì„¤ì •
   - ë‹¨ë½ ê°„ ë¬¸ë§¥ ëŠê¹€ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ 20~50 í† í° ì •ë„ ê²¹ì¹˜ê²Œ ì²˜ë¦¬

4. ë©”íƒ€ë°ì´í„° í¬í•¨
   - ë¬¸ì„œì˜ ì¶œì²˜(URL, ì œëª©) ë“±ë„ ë²¡í„°í™”ì— ë°˜ì˜í•˜ë©´ ì‹ ë¢°ë„ â†‘

5. Token ì˜ˆì‚°ì— ë”°ë¥¸ ì¶œë ¥ ì „ëµ ì¡°ì •
   - ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ë§ì„ìˆ˜ë¡ ë‹µë³€ì˜ Token ì˜ˆì‚°ì€ ì¤„ì–´ë“¦ â†’ ë‹µë³€ ìµœëŒ€ ê¸¸ì´ ì„¤ì • í•„ìš”

ğŸ“ ë§ˆë¬´ë¦¬ ìš”ì•½

RAGëŠ” ë‹¨ìˆœ ìƒì„±í˜• AIì˜ í•œê³„ë¥¼ ë„˜ì–´, ì‚¬ì‹¤ ê¸°ë°˜ ê²€ìƒ‰ì„ ê²°í•©í•œ ê³ í’ˆì§ˆ ì‘ìš©ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” í•µì‹¬ ê¸°ìˆ ì…ë‹ˆë‹¤. LangChainê³¼ LlamaIndexëŠ” ê°ê¸° ë‹¤ë¥¸ ì ‘ê·¼ ë°©ì‹ìœ¼ë¡œ êµ¬í˜„ì„ ë„ì™€ì£¼ë©°, ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œëŠ” êµ¬ì„± ìš”ì†Œ(ì„ë² ë”© ëª¨ë¸, vector DB, ê²€ìƒ‰ ì „ëµ ë“±)ë¥¼ ìƒí™©ì— ë§ê²Œ ì¡°í•©í•˜ëŠ” ê²ƒì´ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.

ë‹¤ìŒ ì ˆì—ì„œëŠ” ì´ëŸ° RAG êµ¬ì„±ì„ ì‹¤ì œ ì›¹/ëª¨ë°”ì¼ ì• í”Œë¦¬ì¼€ì´ì…˜ê³¼ í†µí•©í•˜ëŠ” í”„ë¡ íŠ¸ì—”ë“œ ì—°ê²° ë°©ì‹(Chapter 13)ìœ¼ë¡œ ì´ì–´ì§‘ë‹ˆë‹¤.