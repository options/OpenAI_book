## 9.1 Whisper STT: 음성 → 텍스트 변환

OpenAI의 Whisper는 고성능 음성 인식(Speech-to-Text, STT) 모델로서, 여러 언어의 음성을 텍스트로 변환하는 데 매우 효과적인 결과를 보여줍니다. Whisper는 OpenAI가 2022년에 공개한 오픈소스 STT 모델 중 하나로서, 다양한 언어를 인식할 수 있으며, 잡음이 있는 환경에서도 비교적 우수한 인식률을 자랑합니다. GPT 모델과 같이 Transformer 아키텍처를 기반으로 훈련되었고, 웹사이트, 유튜브, 전화 통화 등 수많은 실제 음성 데이터를 기반으로 학습되었습니다.

이 절에서는 Whisper의 기본 개념, OpenAI API를 통한 사용 방법, 여러 언어 및 포맷에 대한 지원, 코드 예제를 중심으로 Whisper를 음성-to-텍스트 변환에 적용하는 실전 활용 방법을 단계별로 설명합니다.



### 9.1.1 Whisper란 무엇인가?

Whisper는 다음과 같은 특징을 가진 음성 인식 모델입니다:

- 다국어 인식: 영어를 포함한 90개 이상의 언어 지원
- 잡음 환경에서도 강인함: 주변 소음에 덜 민감
- 오픈소스 모델: GitHub를 통해 공개 (https://github.com/openai/whisper)
- 다양한 태스크 지원:
  - 단순한 음성→텍스트 변환
  - 음성 번역 (예: 프랑스어 오디오를 영어로 번역하여 텍스트 생성)
  - 타임스탬프가 포함된 자막 생성 가능

Whisper는 크게 오픈소스로 사용 가능한 로컬 모델과 OpenAI API를 통해 사용 가능한 클라우드 기반 모델로 나뉩니다. 이 절에서는 클라우드 기반 API 중심으로 실습합니다.



### 9.1.2 Whisper STT의 활용 흐름

OpenAI Audio API에서 Whisper STT 기능은 다음과 같은 흐름으로 사용됩니다:

1. 사용자는 음성 파일을 업로드합니다. (최대 25MB까지 업로드 가능)
2. OpenAI API는 해당 오디오 파일을 분석하고 텍스트 결과를 반환합니다.
3. 선택적으로 번역, 타임스탬프 추출, JSON 반환 형식 등의 옵션을 지정할 수 있습니다.

지원되는 대표 입력 포맷:

- .mp3
- .mp4
- .mpeg
- .mpga
- .m4a
- .wav
- .webm



### 9.1.3 API 엔드포인트 및 요청 형식

STT 기능은 OpenAI의 /v1/audio/transcriptions 엔드포인트를 통해 제공됩니다. 이 endpoint는 Whisper 모델을 통해 입력 음성을 텍스트로 변환합니다.

▣ 주요 엔드포인트

- STT 변환 (Transcription):  
  `POST /v1/audio/transcriptions`

- 번역 (Translation):  
  `POST /v1/audio/translations`

▣ 필수 파라미터

| 파라미터 | 설명 |
|----------|------|
| file | 음성 파일 (multipart/form-data 형식) |
| model | 사용 모델 이름 (현재는 "whisper-1"만 사용 가능) |

▣ 선택 파라미터

| 파라미터 | 설명 |
|----------|------|
| prompt | 추가장 정보 제공 용 (ex. 특정 용어 유도) |
| response_format | 'json' (기본), 'text', 'srt', 'verbose_json', 'vtt' |
| temperature | 출력을 다양화 (기본 0.0) |
| language | 입력 언어(ISO-639-1 코드). 생략 시 자동 감지. |



### 9.1.4 Whisper API 호출 예제 (Python)

다음은 OpenAI의 Python SDK를 활용하여 오디오 파일을 텍스트로 변환하는 간단한 예제입니다.

```python
import openai

openai.api_key = "your-api-key"

audio_file = open("sample_audio.mp3", "rb")

transcript = openai.Audio.transcribe(
    model="whisper-1", 
    file=audio_file,
    response_format="json"
)

print(transcript["text"])
```

▣ 주요 설명:

- `"whisper-1"`이 현재 제공되는 Whisper API의 모델 이름입니다.
- `openai.Audio.transcribe()` 함수는 명시적으로 Whisper STT 기능을 수행합니다.
- `response_format="json"` 옵션을 통해 JSON 형식의 응답 결과를 얻습니다.



### 9.1.5 번역 API 호출 예제

Whisper는 번역 기능도 함께 제공합니다. 예를 들어, 프랑스어 음성을 영어 텍스트로 바로 번역하려면:

```python
translated = openai.Audio.translate(
    model="whisper-1",
    file=open("french_speech.mp3", "rb")
)

print(translated["text"])
```

▣ 주의:

- `/translate` 엔드포인트는 음성 언어가 영어가 아닐 때만 사용됩니다.
- Whisper 자체가 음성을 번역할 수 있도록 미리 훈련된 모델을 사용합니다.



### 9.1.6 여러 출력 포맷: 텍스트, JSON, SRT, VTT

Whisper는 다양한 포맷으로 텍스트를 출력할 수 있습니다. 이를 통해 자막 생성, 후처리 용이성 등의 효과를 얻을 수 있습니다.

| 포맷 | 설명 |
|------|------|
| text | 기본 텍스트 (순수 텍스트) |
| json | 구조화된 JSON (추천 포맷) |
| verbose_json | 타임스탬프 포함된 자세한 JSON |
| srt | SubRip 포맷으로 자막 생성 |
| vtt | WebVTT 포맷으로 자막 생성 |

예: SRT 출력 포맷 지정 예

```python
srt_result = openai.Audio.transcribe(
    model="whisper-1",
    file=open("lecture.mp3", "rb"),
    response_format="srt"
)

print(srt_result)
```



### 9.1.7 프롬프트 사용(선택적 힌트 입력)

STT 변환 결과 품질을 높이기 위해 도메인 특화 단어, 인명, 고유명사 등을 "prompt" 파라미터를 통해 사전 힌트로 줄 수 있습니다.

```python
transcript = openai.Audio.transcribe(
    model="whisper-1",
    file=open("interview.mp3", "rb"),
    prompt="이번 대담의 화자는 김지훈 박사이며, 주제는 정보보안입니다."
)
```

이는 특히 특수한 발음, 기술 용어가 많은 경우 인식 품질을 상당히 개선시킬 수 있습니다.



### 9.1.8 대용량 파일 처리 전략

현재 Whisper API는 최대 25MB 크기의 음성 파일만 지원합니다. 따라서 다음과 같은 방법으로 대비해야 합니다:

- 긴 오디오는 25MB 이하로 잘라 처리
- 오디오 길이 기준(예: 5분 단위) 분할
- ffmpeg 같은 도구로 오디오 압축 (MP3 낮은 bitrate 사용)
- Whisper 오픈소스 로컬 버전 사용 (용량 제한 없음)

예: ffmpeg로 오디오 파일 분할 및 압축

```bash
ffmpeg -i input.wav -f segment -segment_time 300 -c copy out%03d.wav
ffmpeg -i out001.wav -b:a 64k out001_compressed.mp3
```



### 9.1.9 Whisper STT 활용 예시

- 회의록 자동 생성기:
  - Zoom, Teams 음성을 STT 변환하여 자동 문서화
- 팟캐스트 스크립트 작성:
  - 녹음된 방송을 텍스트로 변환 후 편집
- 고객센터 통화 녹취 분석:
  - 상담 녹취를 텍스트화하여 키워드 검색, 감정 분석
- 자막 자동 생성기:
  - 동영상 강의에 자동으로 자막(SRT/VTT) 삽입



### 9.1.10 한글 음성에 대한 Whisper의 인식 능력

Whisper는 한국어 인식에도 우수한 성능을 보여줍니다. 특히 다음과 같은 조건에 따라 품질이 결정됩니다:

- 발음의 정확성: 구어체보다는 또렷한 발음이 유리
- 잡음 유무: 환경 소음이 적을수록 정확도 향상
- 전문 용어 포함 시: 'prompt' 파라미터 활용을 권장
- 문장 단위 구분: 긴 문장은 오류가 증가할 수 있음

Whisper는 한국어를 포함한 90개 언어를 자동 감지할 수 있지만, 필요 시 `language="ko"`를 명시하는 것이 정확도를 올리는 데 도움을 줍니다.



### 9.1.11 공통 에러 및 예외 처리

| 에러 메시지 | 원인 | 처리 방법 |
|-------------|------|------------|
| File is too large | 25MB 제한 초과 | 오디오 잘라서 업로드 |
| Unsupported file format | 형식이 이상하거나 손상됨 | MP3, WAV 권장 |
| Rate limit exceeded | 호출 제한 초과 | 백오프(재시도 지연), API 제한 상향 요청 |
| APIKey Invalid | 인증 실패 | API 키 확인 및 보안 관리 필요 |



### 9.1.12 요약 정리

| 항목 | 내용 |
|------|------|
| 사용 API 모델 | "whisper-1" |
| 주요 엔드포인트 | /audio/transcriptions, /audio/translations |
| 입력 포맷 | .mp3, .wav, .m4a 등 |
| 출력 포맷 옵션 | text, json, srt, vtt 등 |
| 언어 자동 감지 | 기본 작동, 수동 설정 가능 (language="ko") |
| 파일 용량 제한 | 최대 25MB |
| 프롬프트 옵션 | 인식 정확도 향상에 도움 |



Whisper API는 사용이 간편하면서 결과 품질도 우수하기 때문에 음성 기반 서비스의 핵심 컴포넌트로 활용할 수 있습니다. 다음 절에서는 Whisper와 TTS(Text-to-Speech)를 결합하여 "음성 입출력 기반 대화 시스템"을 어떻게 구축할 수 있는지 살펴봅니다.