## 2.4 Rate Limit, 토큰 개념 정리

OpenAI API를 실제로 활용하기 위해서는 “토큰(token)”과 “레이트 리밋(Rate Limit)”이라는 두 개념에 대한 명확한 이해가 필수적입니다. 이 장에서는 토큰이 무엇인지, 어떻게 계산되며, 요금이나 요청 제한과 어떤 관계가 있는지를 설명하고, OpenAI의 레이트 리밋 정책과 이를 관리하는 방식에 대해 상세히 살펴보겠습니다.



### 2.4.1 토큰(Token) 개념 이해

생성형 AI 모델에 입력하거나 응답으로 받는 문장은 내부적으로 ‘토큰’이라는 단위로 처리됩니다. 토큰은 모델이 언어를 처리하고 생성하는 가장 작은 단위이며, 이는 반드시 사람이 말하는 단어 하나와 일대일로 대응되지 않습니다.

- 예를 들어:
  - “Hello” → 1개의 토큰
  - “OpenAI is great!” → 5개의 토큰 (“Open”, “AI”, “ is”, “ great”, “!”)
  - 한글은 영어보다 압축률이 낮아 상대적으로 더 많은 토큰을 차지할 수 있습니다.
    - 예: “안녕하세요” → 약 2 ~ 3 토큰
    - 예: “이것은 테스트 문장입니다.” → 7 ~ 10 토큰 정도

즉, OpenAI 모델은 글자(character)가 아닌 언어적으로 의미 있는 단위(morpheme 혹은 subword) 기반의 토큰화를 수행합니다. 이 방식으로 인해 비영어권 언어(한국어 등)는 영어에 비해 동일 문장이라도 더 많은 토큰을 사용할 수 있습니다.

👉 참고: OpenAI는 대부분의 모델에서 클라이브랜드 클리닉(Cleveland Clinic)의 tokenizer인 cl100k_base를 사용합니다. 이 tokenizer는 다양한 언어에서 높은 효율성을 유지하도록 최적화되어 있습니다.  
→ 직접 테스트: [OpenAI Tokenizer 툴](https://platform.openai.com/tokenizer)



### 2.4.2 토큰의 사용처

OpenAI의 API에서 토큰은 다음과 같은 상황에서 사용됩니다.

| 구분              | 설명 |
|------------------|------|
| **입력 토큰 (Prompt Token)** | 사용자가 보낸 메시지 등의 입력 부분 |
| **출력 토큰 (Completion Token)** | 모델이 생성한 응답 결과 |
| **총 토큰 수 (Total Token)** | 입력 + 출력 토큰의 합. 과금 및 rate limit 계산의 기준 |

예시:
```json
입력 프롬프트: "What is the capital of France?"
출력 응답: "The capital of France is Paris."

→ 입력 토큰: 약 8 tokens  
→ 출력 토큰: 약 9 tokens  
→ 총 토큰: 약 17 tokens
```

따라서, 생성하는 응답이 길수록 또는 질문이 길수록 총 토큰 수가 많아지고 요금도 더 발생합니다.



### 2.4.3 최대 토큰 수 제한 (Context Window)

각 모델은 “컨텍스트 윈도우(Context Window)”라는 최대 토큰 허용량을 가집니다. 이 제한 내에서 입력과 출력이 함께 고려됩니다.

| 모델 | 컨텍스트 윈도우 | 비고 |
|------|-----------------|------|
| gpt-3.5-turbo | 4,096 tokens | 저비용, 텍스트 위주 |
| gpt-3.5-turbo-16k | 16,384 tokens | 확장 모델 |
| gpt-4 | 8,192 tokens | 일반 GPT-4 |
| gpt-4-1106-preview (GPT-4o) | 128,000 tokens | 장문 문서 등 대용량 입력 가능 |

- 예를 들어 gpt-4-1106-preview를 사용할 경우, 10만자 이상의 문서를 한번에 처리하는 것도 가능합니다. 다만 출력 토큰도 여기에 포함되어야 하므로, 매우 긴 질문에 대해 너무 긴 응답을 요청하면 오류가 발생할 수 있습니다.

→ 오류 예시: max_tokens 설정이 높고 context 제한 초과 → HTTP 400 error: context length exceeded



### 2.4.4 토큰 사용량과 과금 관계

OpenAI는 API 사용 요금을 “토큰 수 기반”으로 정합니다. 입력과 생성된 출력 토큰 양을 합산하여 과금합니다. 요금은 모델 등급마다 다르며, 공식 문서를 통해 각 모델별 단가를 확인할 수 있습니다.

예시 (2024년 6월 기준, gpt-4o 기준):

| 모델 | 입력 단가 (1K 토큰당) | 출력 단가 (1K 토큰당) |
|---|----------------|----------------|
| gpt-3.5-turbo | $0.0005 | $0.0015 |
| gpt-4 | $0.03 | $0.06 |
| gpt-4-1106-preview (GPT-4o) | $0.005 | $0.015 |

예를 들어:
- 500 입력 토큰 + 500 출력 토큰 = 1,000 토큰 사용
- gpt-4 사용 시 → 입력 $0.03/1K + 출력 $0.06/1K  
→ 총 비용: $0.045

이러한 구조는 사용량 기반 종량제 시스템입니다. 따라서 자동화를 통해 반복 호출 시에는 누적 비용에 주의가 필요합니다.



### 2.4.5 Rate Limit(요청 제한) 정책 이해

OpenAI API에는 몇 가지 종류의 속도 제한(Rate Limit)이 있습니다. 이는 서비스 안정성과 오용 방지를 위한 필요 조치이며, 사용량이 많은 기업이나 앱에서는 반드시 주의해야 할 항목입니다.

OpenAI의 Rate Limit는 다음과 같은 측면으로 나뉩니다:

| 제한 항목 | 설명 |
|----------|------|
| 요청 횟수 제한 (Requests per minute, RPM) | 1분 간 허용되는 최대 요청 횟수 |
| 토큰 처리 제한 (Tokens per minute, TPM) | 1분 간 처리 가능한 입력+출력 토큰 수 |
| 동시 실행 세션 제한 (Concurrency limit) | 하나의 계정이 동시에 진행할 수 있는 요청 스레드 개수 |

예시:
- gpt-4-turbo (기본 tier)
  - RPM: 500
  - TPM: 300,000
  - 동시 처리: 20 threads

→ 즉, 1분간 300,000토큰 이상을 요청하면 다음 요청은 다음 분까지 기다려야 합니다.

Rate Limit은 API의 HTTP 응답 헤더에 함께 반환됩니다.

예:
```http
openai-ratelimit-limit-tpm: 300000
openai-ratelimit-remaining-tpm: 150000
openai-ratelimit-reset-tokens: 2024-06-24T06:25:15Z
```

이를 바탕으로 사용자는 자신의 사용량을 실시간으로 모니터링하고, 초과 여부를 대비할 수 있습니다.



### 2.4.6 Rate Limit 초과 시 대처 방법

Rate Limit을 초과하면 API 호출은 다음과 같은 오류 응답을 보냅니다.

```json
{
  "error": {
    "message": "Rate limit reached for 30000 TPM",
    "type": "rate_limit_error",
    "param": null,
    "code": "429"
  }
}
```

대처법:

1. 요청 빈도를 감소시키거나 일정 시간 대기 후 재요청 (exponential backoff 방식)
2. OpenAI 대시보드에서 현재 사용량 및 제한 확인: https://platform.openai.com/account/usage
3. 필요 시 Rate Limit 상향 요청: 설정 메뉴 → Organization → Usage Limits → “Increase limits” 요청



### 2.4.7 요약: 토큰 & 레이트 리밋 핵심 정리

- “토큰”은 요금과 성능 양면에서 가장 중요한 개념
- 입력 + 출력 = 총 토큰 수 → 과금 기준
- 모델마다 최대 토큰 수(Context Window)가 다르므로 사전 확인 필수
- Rate Limit은 RPM, TPM, 동시 실행 제한으로 구성
- Rate Limit 초과 시 적절한 예외 처리 또는 대기/재시도 로직 필요



다음 장에서는 이러한 토큰과 Rate Limit 구조를 바탕으로 실제 API 호출에서 어떻게 최적화할 수 있는지, 그리고 첫 Python 호출을 어떻게 구현할 수 있을지 다뤄보겠습니다.